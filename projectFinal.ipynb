{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Gathering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the latest versions of said modules upon the creation of this project\n",
    "\n",
    "# Uncomment the line below to install dependencies for the required libraries\n",
    "#!pip install -r requirements.txt\n",
    "\n",
    "# cross check with the cloud based version on the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "**Pandas** is a library that provides many tools for working with data in Python. It is imported to use these tools to performe wide variety of data manipulation and analysis tasks. In this project, pandas is mainly utilized for reading and writing CSV files, including importing the dataset, adding headers, and reviewing its contents using the df.head(n) and df.tail(n) methods. Additionally, functions are used to convert '?' values to \"NaN\" to identify non-numerical values. Pandas is also employed to obtain a brief overview of the dataset's information, data types, and descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 1.5.3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "**NumPy** is a widely used library for Python that simplifies data analysis and scientific computing by providing a variety of useful features for working with numerical data in arrays and matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 1.24.1\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "**matplotlib** is a module within the matplotlib library in Python that is utilized for making plots and visualizing data. It offers a user-friendly interface to create a variety of plots, such as line plots, scatter plots, bar plots, and histograms. \n",
    "\n",
    "// pylab & pylot are used; explain each briefly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 3.6.3\n",
    "import matplotlib.pylab as plt_lab\n",
    "import matplotlib.pyplot as plt_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn\n",
    "**Seaborn** is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn is used to create a heatmap to visualize the correlation between the features in the dataset.\n",
    "\n",
    "// copilot generated text; needs to proofread and revised! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn 0.12.2\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy\n",
    "**SciPy** is a Python library that is used to perform scientific computing and technical computing. It is used to perform a variety of mathematical, scientific, and engineering tasks. In this project, SciPy is used to perform a one-way ANOVA test to determine if there is a significant difference in the means of the three groups.\n",
    "\n",
    "// copilot generated text; needs to proofread and revised! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy 1.2.1\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "**Scikit-learn** is a Python library that is used for machine learning. It is used to perform a variety of machine learning tasks, such as classification, regression, and clustering. In this project, Scikit-learn is used to perform a logistic regression to predict the class of a new instance.\n",
    "\n",
    "// copilot generated text; needs to proofread and revised! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn 1.2.1\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ipywidgets\n",
    "**Ipywidgets** is a Python library that is used to create interactive widgets for Jupyter notebooks. It is used to create a slider to select the number of features to be used in the logistic regression model.\n",
    "\n",
    "// copilot generated text; needs to proofread and revised!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipywidgets 8.0.4\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tqdm\n",
    "**tqdm** is a Python library that is used to create progress bars for loops. It is used to create a progress bar to show the progress of the logistic regression model.\n",
    "\n",
    "// copilot generated text; needs to proofread and revised!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm 4.64.1\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. 1985 Auto Imports Database\n",
    "This dataset is donated by Jeffrey C. Schlimmer on May 19, 1987. The dataset's source are:\n",
    "1. 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook\n",
    "2. Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038\n",
    "3. Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "The attributes of this dataset represent:\n",
    "- specification of an auto in terms of various characteristics\n",
    "- assigned insurance risk rating (symboling)\n",
    "- normalized losses in use as compared to other cars\n",
    "\n",
    "    *This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **symboling** - the degree to which the auto is more risky than its price indicates (3 = very risky, -3 = very safe)\n",
    "2. **normalized-losses** - normalized loss value for a particular car; may represent the financial loss or cost associated with a car\n",
    "3. **make** - make of the car (e.g., Honda, Toyota, etc.)\n",
    "4. **fuel-type** - type of fuel used by the car (gas or diesel)\n",
    "5. **aspiration** - type of engine aspiration (standard or turbocharged)\n",
    "6. **num-of-doors** - number of doors on the car \n",
    "7. **body-style** - style of the car's body (e.g., sedan, hatchback, etc.)\n",
    "8. **drive-wheels** - configuration of the car's drive wheels (front, rear, or four-wheel)\n",
    "9. **engine-location** - location of the car's engine (front or rear)\n",
    "10. **wheel-base** - distance between the centers of the front and rear wheels \n",
    "11. **length** - length of the car \n",
    "12. **width** - width of the car \n",
    "13. **height** - height of the car\n",
    "14. **curb-weight** - weight of the car without any passengers or cargo\n",
    "15. **engine-type** - type of engine in the car (e.g., dohc, ohc, etc.)\n",
    "16. **num-of-cylinders** - number of cylinders in the car's engine\n",
    "17. **engine-size** - size of the car's engine\n",
    "18. **fuel-system** - fuel system used by the car (e.g., mpfi, 2bbl, etc.)\n",
    "19. **bore** - diameter of the car's cylinders\n",
    "20. **stroke** - distance traveled by the car's pistons during one engine cycle\n",
    "21. **compression-ratio** - ratio of the volume of the combustion chamber at the bottom of the piston stroke to the volume at the top\n",
    "22. **horsepower** - power of the car's engine\n",
    "23. **peak-rpm** - highest number of revolutions per minute the car's engine can make\n",
    "24. **city-mpg** - number of miles per gallon the car can get in the city\n",
    "25. **highway-mpg** - number of miles per gallon the car can get on the highway\n",
    "26. **price** - price of the car"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace dataset headers with attribute names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns to their proper attribute labels\n",
    "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
    "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
    "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
    "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and save the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"./auto.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset, header=None, names=headers)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Basic insight on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset has\", df.shape[0] , \"rows and\", df.shape[1], \"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The types of the columns are as follows:\\n\", df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe\n",
    "This provides a statistical summary of all columns, including object-typed attributes.\n",
    "\n",
    "*Note: Some values in the table below show as \"NaN\" because their respective columns contain missing data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = \"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info\n",
    "This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Cleansing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Identify and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"?\" to NaN\n",
    "df.replace(\"?\", np.nan, inplace = True)\n",
    "\n",
    "# Identify missing values\n",
    "print(\"The number of missing values per column are:\\n\", df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are several ways to deal with missing data:\n",
    "1. **Drop data** - a data point (row) is removed if it contains a missing value\n",
    "2. **Replace data** - missing value is replaced by mean or frequency\n",
    "3. **Drop attribute** - the entire column is removed if it contains enough missing values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Losses\n",
    "The normalized losses attribute has 41 missing values, which is about 20% of the total dataset, which is insignificant enough for the attribute to be dropped. Furthermore, since it is a numerical variable and the data distribution is barely asymmetrical, replacing the missing values with the mean is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of normalized losses\n",
    "plt_lab.hist(df[\"normalized-losses\"].astype(\"float\"), bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean value for normalized losses\n",
    "mean_normalized_losses = df[\"normalized-losses\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of normalized losses:\", mean_normalized_losses)\n",
    "\n",
    "# replace NaN with mean value in \"normalized-losses\" column\n",
    "df[\"normalized-losses\"].replace(np.nan, mean_normalized_losses, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num of Doors\n",
    "The num of doors attribute has only 2 missing values. It is a categorical variable, so replacing the missing values with the most frequent value is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the frequency of each value in the \"num-of-doors\" column\n",
    "print(\"The frequency of each value in the num-of-doors column is:\\n\", df[\"num-of-doors\"].value_counts())\n",
    "\n",
    "# replace the missing 'num-of-doors' values by the most frequent\n",
    "df[\"num-of-doors\"].replace(np.nan, \"four\", inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bore\n",
    "The bore attribute has only 4 missing values. It is a numerical variable, so replacing the missing values with the mean is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean value for bore\n",
    "mean_bore = df[\"bore\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of bore:\", mean_bore)\n",
    "\n",
    "# replace NaN with mean value in \"bore\" column\n",
    "df[\"bore\"].replace(np.nan, mean_bore, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroke\n",
    "The stroke attribute has only 4 missing values. It is a numerical variable, so replacing the missing values with the mean is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean value for stroke\n",
    "mean_stroke = df[\"stroke\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of stroke:\", mean_stroke)\n",
    "\n",
    "# replace NaN with mean value in \"stroke\" column\n",
    "df[\"stroke\"].replace(np.nan, mean_stroke, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horsepower\n",
    "The horsepower attribute has only 2 missing values. It is a numerical variable, so replacing the missing values with the mean is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean value for horsepower\n",
    "mean_horsepower = df[\"horsepower\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average horsepower:\", mean_horsepower)\n",
    "\n",
    "# replace NaN with mean value in \"horsepower\" column\n",
    "df[\"horsepower\"].replace(np.nan, mean_horsepower, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak RPM\n",
    "The peak rpm attribute has only 2 missing values. It is a numerical variable, so replacing the missing values with the mean is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean value for peak-rpm\n",
    "mean_peak_rpm = df[\"peak-rpm\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average peak rpm:\", mean_peak_rpm)\n",
    "\n",
    "# replace NaN with mean value in \"peak-rpm\" column\n",
    "df[\"peak-rpm\"].replace(np.nan, mean_peak_rpm, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price\n",
    "The price attribute has only 4 missing values. However, it is the target variable. As such, the rows with missing values are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that do not have price data\n",
    "df.dropna(subset=[\"price\"], axis=0, inplace=True)\n",
    "\n",
    "# reset index, because we droped two rows\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Data standardization\n",
    "In the context of the auto dataset provided, data standardization could be applied to the variables in order to make them comparable and interpretable. For example, the variables \"normalized-losses\", \"wheel-base\", \"length\", \"width\", \"height\", etc. are measured in different units and scales, which can make it difficult to compare and analyze them. By standardizing these variables, we can transform them into a common format, such as z-scores, which makes it easier to compare and analyze them.\n",
    "\n",
    "Also, standardizing the data can help to identify the relationship between the independent and dependent variable. For instance, using the mean subtraction and division by the standard deviation method, can make the data more easily comparable and interpretable, by scaling the data and making it more easily comparable to other variables in the dataset. This can be useful in identifying patterns and relationships in the data that would not be easily apparent if the variables were in their original format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting the data format of the columns\n",
    "It has been observed that some columns in the dataset have incorrect data types. To fix this issue, the \"astype()\" method will be used to convert the data types of each column to the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert attributes to correct data format\n",
    "df[[\"bore\", \"stroke\"]] = df[[\"bore\", \"stroke\"]].astype(\"float\")\n",
    "df[[\"normalized-losses\"]] = df[[\"normalized-losses\"]].astype(\"int\")\n",
    "df[[\"price\"]] = df[[\"price\"]].astype(\"float\")\n",
    "df[[\"peak-rpm\"]] = df[[\"peak-rpm\"]].astype(\"float\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Conversion\n",
    "In order to compare the fuel economy across different vehicles, it is important to have a standard unit of measurement so that the values can be accurately compared. L/100km is an internationally recognized standard unit of measurement for fuel consumption, therefore it can be useful to convert mpg to L/100km in order to facilitate more accurate comparisons between vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mpg to L/100km by mathematical operation (235 divided by mpg) \n",
    "df['city-L/100km'] = 235/df[\"city-mpg\"]\n",
    "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\n",
    "\n",
    "# drop the original city-mpg and highway-mpg columns\n",
    "df.drop(['city-mpg', 'highway-mpg'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Data normalization\n",
    "In the context of the auto dataset provided, normalization can be useful when comparing variables that have different scales and units of measurement. For example, \"normalized-losses\", \"wheel-base\", \"length\", \"width\", \"height\" etc are measured in different units and scales, so in order to compare them effectively and make meaningful analysis, normalizing the variables so that they have similar ranges can be necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace original value by (original value)/(maximum value)\n",
    "df['length'] = df['length']/df['length'].max()\n",
    "df['width'] = df['width']/df['width'].max()\n",
    "df['height'] = df['height']/df['height'].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Indicator variable\n",
    "It would be beneficial to utilize indicator variables for the \"fuel type\" and \"aspiration\" attributes within the automobile dataset as it would enable the inclusion of these categorical variables in regression analysis, thus allowing for a more comprehensive examination of their impact on the dependent variable(s) of interest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for fuel-type\n",
    "dummy_variable_1 = pd.get_dummies(df[\"fuel-type\"])\n",
    "dummy_variable_1.rename(columns={'gas':'fuel-type-gas', 'diesel':'fuel-type-diesel'}, inplace=True)\n",
    "\n",
    "# merge the dummy variables into the original dataframe\n",
    "df = pd.concat([df, dummy_variable_1], axis=1)\n",
    "\n",
    "# drop the original column \"fuel-type\" from \"df\"\n",
    "df.drop(\"fuel-type\", axis = 1, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for aspiration\n",
    "dummy_variable_2 = pd.get_dummies(df['aspiration'])\n",
    "dummy_variable_2.rename(columns={'std':'aspiration-std', 'turbo':'aspiration-turbo'}, inplace=True)\n",
    "\n",
    "# merge the dummy variables into the original dataframe\n",
    "df = pd.concat([df, dummy_variable_2], axis=1)\n",
    "\n",
    "# drop the original column \"aspiration\" from \"df\"\n",
    "df.drop('aspiration', axis = 1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing individual feature patterns using visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Numerical Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
